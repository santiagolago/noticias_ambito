================================================================================
                    PROYECTO SCRAPY - NOTICIAS ECON√ìMICAS
================================================================================

================================================================================
1. ¬øQU√â ES UN PROYECTO SCRAPY?
================================================================================

Scrapy es un framework de web scraping que organiza el c√≥digo en una estructura
de proyecto modular. La diferencia clave es:

PROYECTO SCRAPY (carpeta "noticias_econ/")
‚îÇ
‚îú‚îÄ‚îÄ scrapy.cfg              ‚Üí Archivo de configuraci√≥n de despliegue
‚îú‚îÄ‚îÄ noticias_econ/          ‚Üí M√≥dulo Python del proyecto
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py         ‚Üí Marca la carpeta como paquete Python
‚îÇ   ‚îú‚îÄ‚îÄ settings.py         ‚Üí ‚≠ê Configuraci√≥n global del proyecto
‚îÇ   ‚îú‚îÄ‚îÄ items.py            ‚Üí Define estructura de datos (opcional)
‚îÇ   ‚îú‚îÄ‚îÄ middlewares.py      ‚Üí Procesamiento de requests/responses
‚îÇ   ‚îú‚îÄ‚îÄ pipelines.py        ‚Üí Procesamiento post-scraping de items
‚îÇ   ‚îî‚îÄ‚îÄ spiders/            ‚Üí üìÇ Carpeta con los spiders
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ ambito.py       ‚Üí ‚≠ê TU SPIDER (c√≥digo de scraping)


FUNCI√ìN DE CADA ARCHIVO:
------------------------

- scrapy.cfg
  ‚Üí Configuraci√≥n de despliegue. Indica d√≥nde est√° el m√≥dulo settings.
  ‚Üí Raramente necesitas modificarlo.

- settings.py
  ‚Üí ‚≠ê CONFIGURACI√ìN GLOBAL: velocidad, user-agent, formato de exportaci√≥n,
     comportamiento del bot, etc.
  ‚Üí Afecta a TODOS los spiders del proyecto.

- items.py
  ‚Üí Define la estructura de datos que vas a scrapear (como un schema).
  ‚Üí Opcional: puedes usar diccionarios directamente (como en ambito.py).

- middlewares.py
  ‚Üí Intercepta y modifica requests antes de enviarlos y responses antes de
     procesarlos.
  ‚Üí √ötil para: rotar proxies, manejar headers, bypass anti-bot, etc.

- pipelines.py
  ‚Üí Procesa items DESPU√âS de ser scrapeados.
  ‚Üí √ötil para: limpiar datos, guardar en bases de datos, validar, etc.

- spiders/ambito.py
  ‚Üí ‚≠ê TU C√ìDIGO DE SCRAPING: la l√≥gica espec√≠fica para extraer datos de
     ambito.com.
  ‚Üí Cada spider es independiente y puede tener su propia l√≥gica.


ANALOG√çA:
---------
Proyecto Scrapy = una f√°brica completa
Spider (ambito.py) = un trabajador espec√≠fico que sabe c√≥mo procesar un material
Settings = las reglas y configuraci√≥n de la f√°brica
Pipelines = la l√≠nea de ensamblaje que procesa el producto final


================================================================================
2. WORKFLOW DEL C√ìDIGO - VISTA PANOR√ÅMICA
================================================================================

INICIO: scrapy crawl ambito
    ‚Üì
[1] Spider arranca en: https://www.ambito.com/economia
    ‚Üì
[2] M√©todo parse() extrae links de noticias
    ‚Üì
[3] Sigue cada link ‚Üí parse_noticia() extrae: t√≠tulo, fecha, cuerpo
    ‚Üì
[4] Genera item (diccionario) y lo yielda
    ‚Üì
[5] parse() avanza a siguiente p√°gina: /economia/1, /economia/2, etc.
    ‚Üì
[6] Repite [2-5] hasta encontrar una p√°gina SIN noticias
    ‚Üì
FIN: Exporta todos los items a noticias.csv


================================================================================
3. WORKFLOW DEL C√ìDIGO - DETALLADO
================================================================================

CLASE AmbitoSpider (hereda de scrapy.Spider)
--------------------------------------------

ATRIBUTOS:
- name = "ambito"                    ‚Üí Identificador √∫nico del spider
- allowed_domains = ["ambito.com"]   ‚Üí Solo scrapear√° URLs de este dominio
- start_urls = [...]                 ‚Üí Primera URL a visitar


M√âTODO __init__()
-----------------
Se ejecuta al iniciar el spider.

    self.current_page = 0

‚Üí Inicializa contador de p√°ginas en 0 (p√°gina inicial sin n√∫mero en URL)


M√âTODO parse(response)
----------------------
Se ejecuta AUTOM√ÅTICAMENTE cuando Scrapy visita una p√°gina de listado.

    news_links = response.css("h2.news-article__title a::attr(href)").getall()

‚Üí Extrae TODOS los atributos href de los links dentro de <h2> con clase
  "news-article__title"
‚Üí Ejemplo de HTML que busca:
  <h2 class="news-article__title">
      <a href="/economia/noticia-123">T√≠tulo</a>
  </h2>

‚Üí getall() devuelve una LISTA de URLs: ['/economia/not-1', '/economia/not-2', ...]


    if not news_links:
        self.logger.info("No hay m√°s noticias, finalizando scraper")
        return

‚Üí CONDICI√ìN DE PARADA: Si la lista est√° vac√≠a, significa que llegamos al
  final de las noticias disponibles.
‚Üí return ‚Üí detiene el m√©todo sin generar m√°s requests


    for link in news_links:
        yield response.follow(link, callback=self.parse_noticia)

‚Üí ITERA cada link encontrado
‚Üí response.follow() ‚Üí sigue el link (puede ser relativo, Scrapy lo completa)
‚Üí callback=self.parse_noticia ‚Üí cuando llegue la respuesta, ejecuta ese m√©todo
‚Üí yield ‚Üí devuelve el request al motor de Scrapy para que lo procese


    self.current_page += 1
    next_page_url = f"https://www.ambito.com/economia/{self.current_page}"
    yield scrapy.Request(next_page_url, callback=self.parse)

‚Üí Incrementa el contador de p√°ginas
‚Üí Construye la URL de la siguiente p√°gina (/economia/1, /economia/2, etc.)
‚Üí Genera un nuevo request para visitar esa p√°gina
‚Üí callback=self.parse ‚Üí cuando llegue, ejecuta nuevamente parse()
‚Üí ESTO CREA UN LOOP: parse ‚Üí sigue links ‚Üí avanza p√°gina ‚Üí parse ‚Üí ...


M√âTODO parse_noticia(response)
-------------------------------
Se ejecuta cuando Scrapy visita una noticia individual.

    titulo = response.css("h1.news-headline__title::text").get()

‚Üí Extrae el texto del <h1> con clase "news-headline__title"
‚Üí ::text ‚Üí solo extrae el texto, no las etiquetas HTML
‚Üí .get() ‚Üí devuelve el PRIMER resultado (o None si no encuentra nada)


    fecha = response.css("span.news-headline__publication-date::text").get()

‚Üí Similar al t√≠tulo, extrae la fecha de un <span>


    cuerpo = response.xpath(
        "string(//article[contains(@class,'article-body')])"
    ).get()

‚Üí USA XPATH en lugar de CSS (m√°s poderoso para este caso)
‚Üí string() ‚Üí funci√≥n XPath que extrae TODO el texto de un nodo y sus hijos
‚Üí //article[contains(@class,'article-body')] ‚Üí busca cualquier <article>
  que contenga "article-body" en su clase
‚Üí Esto EVITA el problema de fragmentaci√≥n de texto cuando hay <strong>,
  <em>, etc. dentro de los p√°rrafos


    if cuerpo:
        cuerpo = " ".join(cuerpo.split())

‚Üí Si hay cuerpo, normaliza espacios en blanco
‚Üí .split() ‚Üí divide por cualquier whitespace (espacios, \n, \t) ‚Üí lista
‚Üí " ".join() ‚Üí une la lista con un solo espacio
‚Üí Resultado: "texto    con\nmuchos\t  espacios" ‚Üí "texto con muchos espacios"


    if titulo and cuerpo:
        yield {
            "url": response.url,
            "titulo": titulo.strip() if titulo else None,
            "fecha": " ".join(fecha.split()) if fecha else None,
            "cuerpo": cuerpo,
        }

‚Üí VALIDACI√ìN: solo devuelve el item si tiene t√≠tulo Y cuerpo
‚Üí yield ‚Üí devuelve un diccionario (item) al motor de Scrapy
‚Üí Scrapy lo procesa seg√∫n settings.py y lo exporta al CSV


================================================================================
4. SETTINGS ELEGIDOS - EXPLICACI√ìN
================================================================================

BOT_NAME = "noticias_econ"
--------------------------
‚Üí Nombre identificador del proyecto
‚Üí Se usa internamente por Scrapy


SPIDER_MODULES y NEWSPIDER_MODULE
----------------------------------
‚Üí Indica d√≥nde Scrapy debe buscar los spiders
‚Üí SPIDER_MODULES: d√≥nde est√°n los spiders existentes
‚Üí NEWSPIDER_MODULE: d√≥nde crear nuevos spiders con `scrapy genspider`


ROBOTSTXT_OBEY = True
---------------------
‚Üí ‚≠ê √âTICA DE SCRAPING: respeta el archivo robots.txt del sitio
‚Üí Si ambito.com tiene reglas que proh√≠ben scrapear ciertas p√°ginas, las respeta
‚Üí SIEMPRE d√©jalo en True a menos que tengas permiso expl√≠cito del sitio


CONCURRENT_REQUESTS = 16 (comentado, default)
----------------------------------------------
‚Üí N√∫mero de requests simult√°neos que Scrapy puede hacer
‚Üí Default: 16 (buen balance entre velocidad y no saturar el servidor)
‚Üí ‚ö†Ô∏è No aumentes mucho este valor o podr√≠as ser bloqueado


CONCURRENT_REQUESTS_PER_DOMAIN = 1
-----------------------------------
‚Üí ‚≠ê CLAVE PARA NO SER BLOQUEADO
‚Üí Solo 1 request a la vez a ambito.com
‚Üí Evita saturar el servidor y parecer un bot malicioso
‚Üí El scraping ser√° m√°s lento pero m√°s seguro


DOWNLOAD_DELAY = 1
------------------
‚Üí ‚≠ê ESPERA 1 SEGUNDO entre cada request
‚Üí Simula comportamiento humano
‚Üí Reduce la carga en el servidor
‚Üí Disminuye probabilidad de ser bloqueado
‚Üí Puedes aumentarlo a 2-3 si quieres ser m√°s conservador


custom_settings - FEEDS
------------------------
‚Üí Configuraci√≥n de exportaci√≥n de datos

'FEEDS': {
    'noticias.csv': {
        'format': 'csv',              ‚Üí Formato de salida
        'encoding': 'utf-8',          ‚Üí Para caracteres especiales (√±, √°, etc.)
        'store_empty': False,         ‚Üí No crea archivo si no hay items
        'fields': [...],              ‚Üí Orden de las columnas en el CSV
        'overwrite': True,            ‚Üí Sobreescribe el archivo si existe
    }
}

‚Üí Esto SIMPLIFICA la ejecuci√≥n: no necesitas especificar -o noticias.csv


'FEED_EXPORT_FIELDS': ['url', 'titulo', 'fecha', 'cuerpo']
-----------------------------------------------------------
‚Üí Garantiza que el CSV tenga estas columnas EN ESTE ORDEN
‚Üí Importante para consistencia cuando proceses los datos despu√©s


================================================================================
5. C√ìMO EJECUTAR EL BOT
================================================================================

PASO 1: Abre la terminal
-------------------------

PASO 2: Navega al directorio del PROYECTO (donde est√° scrapy.cfg)
------------------------------------------------------------------

Si est√°s en tu carpeta de usuario:

    Windows:
    cd C:\Users\TuUsuario\ruta\al\proyecto\noticias_econ

    Mac/Linux:
    cd ~/ruta/al/proyecto/noticias_econ


‚ö†Ô∏è IMPORTANTE: Debes estar en la carpeta que CONTIENE el archivo scrapy.cfg
   y la subcarpeta noticias_econ/

Verifica que est√°s en el lugar correcto:

    Windows:
    dir

    Mac/Linux:
    ls

Deber√≠as ver:
    scrapy.cfg
    noticias_econ/


PASO 3: Ejecuta el spider
--------------------------

Comando b√°sico:

    scrapy crawl ambito


Con el settings.py configurado, esto autom√°ticamente:
‚úì Scrapear√° todas las p√°ginas de econom√≠a
‚úì Exportar√° a noticias.csv en la carpeta actual
‚úì Respetar√° el delay de 1 segundo entre requests


COMANDOS OPCIONALES:
--------------------

- Ver m√°s informaci√≥n durante el scraping:

    scrapy crawl ambito --loglevel=INFO


- Ver TODO el detalle (debugging):

    scrapy crawl ambito --loglevel=DEBUG


- Exportar a otro archivo (sobrescribe configuraci√≥n):

    scrapy crawl ambito -o otro_nombre.csv


- Exportar a JSON en lugar de CSV:

    scrapy crawl ambito -o noticias.json


- Limitar p√°ginas (√∫til para pruebas):

    scrapy crawl ambito -a max_pages=3

  (Nota: necesitar√≠as agregar esta funcionalidad al c√≥digo)

  codigo modificado para que scrapee solo 3 p√°ginas:
  def __init__(self, max_pages=None, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.current_page = 0
    # Si no se especifica max_pages, usa un valor muy alto (infinito efectivo)
    self.max_pages = int(max_pages) if max_pages else 999999

def parse(self, response):
    news_links = response.css("h2.news-article__title a::attr(href)").getall()
    self.logger.info(f"P√°gina {self.current_page}: {len(news_links)} links encontrados")

    if not news_links:
        self.logger.info("No hay m√°s noticias, finalizando scraper")
        return

    for link in news_links:
        yield response.follow(link, callback=self.parse_noticia)

    # ‚≠ê Agregar condici√≥n de max_pages
    self.current_page += 1
    if self.current_page < self.max_pages:  # ‚≠ê ESTA ES LA L√çNEA CLAVE
        next_page_url = f"https://www.ambito.com/economia/{self.current_page}"
        self.logger.info(f"Avanzando a la p√°gina {self.current_page}")
        yield scrapy.Request(next_page_url, callback=self.parse)
    else:
        self.logger.info(f"L√≠mite de {self.max_pages} p√°ginas alcanzado")


PASO 4: Espera a que termine
-----------------------------

El spider se detendr√° autom√°ticamente cuando encuentre una p√°gina sin noticias.

Ver√°s en la terminal:
    [ambito] INFO: No hay m√°s noticias, finalizando scraper
    [scrapy.core.engine] INFO: Closing spider (finished)


PASO 5: Encuentra tu archivo
-----------------------------

El archivo noticias.csv estar√° en el mismo directorio donde ejecutaste el comando.


================================================================================
6. TROUBLESHOOTING
================================================================================

PROBLEMA: "ModuleNotFoundError: No module named 'noticias_econ'"
SOLUCI√ìN: Est√°s en el directorio incorrecto. Ve a la carpeta que contiene
          scrapy.cfg


PROBLEMA: "Spider not found: ambito"
SOLUCI√ìN: Verifica que el archivo ambito.py est√© en noticias_econ/spiders/
          y que name = "ambito" est√© correctamente definido


PROBLEMA: El scraping es muy lento
SOLUCI√ìN: Es normal por DOWNLOAD_DELAY = 1. Puedes reducirlo a 0.5 pero
          aumenta el riesgo de bloqueo.


PROBLEMA: No se exporta el CSV
SOLUCI√ìN: Verifica que hay items scrapeados. Si todas las noticias fallan
          la validaci√≥n (no tienen t√≠tulo o cuerpo), no se crea el archivo.


PROBLEMA: Caracteres raros en el CSV (√É¬±, √É¬©, etc.)
SOLUCI√ìN: Abre el CSV con un editor que soporte UTF-8 (VS Code, Sublime)
          o en Excel: Datos > Desde texto > selecciona UTF-8


================================================================================
7. PR√ìXIMOS PASOS
================================================================================

- Agregar m√°s campos: categor√≠a, autor, im√°genes
- Implementar pipelines para guardar en base de datos (MongoDB, PostgreSQL)
- Agregar manejo de errores m√°s robusto
- Implementar rotaci√≥n de user-agents
- Scrapear otras secciones: pol√≠tica, deportes, etc.
- Programar ejecuciones autom√°ticas (cron, Task Scheduler)


================================================================================
AUTOR: [Tu nombre]
FECHA: Enero 2026
VERSI√ìN: 1.0
================================================================================

Tutorial scraping en la Virtual Machine






Todos los comandos de Git se corren en la carpeta ra√≠z del proyecto, es decir, la carpeta que contiene scrapy.cfg.


