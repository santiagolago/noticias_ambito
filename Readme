================================================================================
                    PROYECTO SCRAPY - NOTICIAS ECON√ìMICAS
================================================================================

================================================================================
1. ¬øQU√â ES UN PROYECTO SCRAPY?
================================================================================

Scrapy es un framework de web scraping que organiza el c√≥digo en una estructura
de proyecto modular. La diferencia clave es:

PROYECTO SCRAPY (carpeta "noticias_econ/")
‚îÇ
‚îú‚îÄ‚îÄ scrapy.cfg              ‚Üí Archivo de configuraci√≥n de despliegue
‚îú‚îÄ‚îÄ noticias_econ/          ‚Üí M√≥dulo Python del proyecto
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py         ‚Üí Marca la carpeta como paquete Python
‚îÇ   ‚îú‚îÄ‚îÄ settings.py         ‚Üí ‚≠ê Configuraci√≥n global del proyecto
‚îÇ   ‚îú‚îÄ‚îÄ items.py            ‚Üí Define estructura de datos (opcional)
‚îÇ   ‚îú‚îÄ‚îÄ middlewares.py      ‚Üí Procesamiento de requests/responses
‚îÇ   ‚îú‚îÄ‚îÄ pipelines.py        ‚Üí Procesamiento post-scraping de items
‚îÇ   ‚îî‚îÄ‚îÄ spiders/            ‚Üí üìÇ Carpeta con los spiders
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ ambito.py       ‚Üí ‚≠ê TU SPIDER (c√≥digo de scraping)


FUNCI√ìN DE CADA ARCHIVO:
------------------------

- scrapy.cfg
  ‚Üí Configuraci√≥n de despliegue. Indica d√≥nde est√° el m√≥dulo settings.
  ‚Üí Raramente necesitas modificarlo.

- settings.py
  ‚Üí ‚≠ê CONFIGURACI√ìN GLOBAL: velocidad, user-agent, formato de exportaci√≥n,
     comportamiento del bot, etc.
  ‚Üí Afecta a TODOS los spiders del proyecto.

- items.py
  ‚Üí Define la estructura de datos que vas a scrapear (como un schema).
  ‚Üí Opcional: puedes usar diccionarios directamente (como en ambito.py).

- middlewares.py
  ‚Üí Intercepta y modifica requests antes de enviarlos y responses antes de
     procesarlos.
  ‚Üí √ötil para: rotar proxies, manejar headers, bypass anti-bot, etc.

- pipelines.py
  ‚Üí Procesa items DESPU√âS de ser scrapeados.
  ‚Üí √ötil para: limpiar datos, guardar en bases de datos, validar, etc.

- spiders/ambito.py
  ‚Üí ‚≠ê TU C√ìDIGO DE SCRAPING: la l√≥gica espec√≠fica para extraer datos de
     ambito.com.
  ‚Üí Cada spider es independiente y puede tener su propia l√≥gica.


ANALOG√çA:
---------
Proyecto Scrapy = una f√°brica completa
Spider (ambito.py) = un trabajador espec√≠fico que sabe c√≥mo procesar un material
Settings = las reglas y configuraci√≥n de la f√°brica
Pipelines = la l√≠nea de ensamblaje que procesa el producto final


================================================================================
2. WORKFLOW DEL C√ìDIGO - VISTA PANOR√ÅMICA
================================================================================

INICIO: scrapy crawl ambito
    ‚Üì
[1] Spider arranca en: https://www.ambito.com/economia
    ‚Üì
[2] M√©todo parse() extrae links de noticias
    ‚Üì
[3] Sigue cada link ‚Üí parse_noticia() extrae: t√≠tulo, fecha, cuerpo
    ‚Üì
[4] Genera item (diccionario) y lo yielda
    ‚Üì
[5] parse() avanza a siguiente p√°gina: /economia/1, /economia/2, etc.
    ‚Üì
[6] Repite [2-5] hasta encontrar una p√°gina SIN noticias
    ‚Üì
FIN: Exporta todos los items a noticias.csv


================================================================================
3. WORKFLOW DEL C√ìDIGO - DETALLADO
================================================================================

CLASE AmbitoSpider (hereda de scrapy.Spider)
--------------------------------------------

ATRIBUTOS:
- name = "ambito"                    ‚Üí Identificador √∫nico del spider
- allowed_domains = ["ambito.com"]   ‚Üí Solo scrapear√° URLs de este dominio
- start_urls = [...]                 ‚Üí Primera URL a visitar


M√âTODO __init__()
-----------------
Se ejecuta al iniciar el spider.

    self.current_page = 0

‚Üí Inicializa contador de p√°ginas en 0 (p√°gina inicial sin n√∫mero en URL)


M√âTODO parse(response)
----------------------
Se ejecuta AUTOM√ÅTICAMENTE cuando Scrapy visita una p√°gina de listado.

    news_links = response.css("h2.news-article__title a::attr(href)").getall()

‚Üí Extrae TODOS los atributos href de los links dentro de <h2> con clase
  "news-article__title"
‚Üí Ejemplo de HTML que busca:
  <h2 class="news-article__title">
      <a href="/economia/noticia-123">T√≠tulo</a>
  </h2>

‚Üí getall() devuelve una LISTA de URLs: ['/economia/not-1', '/economia/not-2', ...]


    if not news_links:
        self.logger.info("No hay m√°s noticias, finalizando scraper")
        return

‚Üí CONDICI√ìN DE PARADA: Si la lista est√° vac√≠a, significa que llegamos al
  final de las noticias disponibles.
‚Üí return ‚Üí detiene el m√©todo sin generar m√°s requests


    for link in news_links:
        yield response.follow(link, callback=self.parse_noticia)

‚Üí ITERA cada link encontrado
‚Üí response.follow() ‚Üí sigue el link (puede ser relativo, Scrapy lo completa)
‚Üí callback=self.parse_noticia ‚Üí cuando llegue la respuesta, ejecuta ese m√©todo
‚Üí yield ‚Üí devuelve el request al motor de Scrapy para que lo procese


    self.current_page += 1
    next_page_url = f"https://www.ambito.com/economia/{self.current_page}"
    yield scrapy.Request(next_page_url, callback=self.parse)

‚Üí Incrementa el contador de p√°ginas
‚Üí Construye la URL de la siguiente p√°gina (/economia/1, /economia/2, etc.)
‚Üí Genera un nuevo request para visitar esa p√°gina
‚Üí callback=self.parse ‚Üí cuando llegue, ejecuta nuevamente parse()
‚Üí ESTO CREA UN LOOP: parse ‚Üí sigue links ‚Üí avanza p√°gina ‚Üí parse ‚Üí ...


M√âTODO parse_noticia(response)
-------------------------------
Se ejecuta cuando Scrapy visita una noticia individual.

    titulo = response.css("h1.news-headline__title::text").get()

‚Üí Extrae el texto del <h1> con clase "news-headline__title"
‚Üí ::text ‚Üí solo extrae el texto, no las etiquetas HTML
‚Üí .get() ‚Üí devuelve el PRIMER resultado (o None si no encuentra nada)


    fecha = response.css("span.news-headline__publication-date::text").get()

‚Üí Similar al t√≠tulo, extrae la fecha de un <span>


    cuerpo = response.xpath(
        "string(//article[contains(@class,'article-body')])"
    ).get()

‚Üí USA XPATH en lugar de CSS (m√°s poderoso para este caso)
‚Üí string() ‚Üí funci√≥n XPath que extrae TODO el texto de un nodo y sus hijos
‚Üí //article[contains(@class,'article-body')] ‚Üí busca cualquier <article>
  que contenga "article-body" en su clase
‚Üí Esto EVITA el problema de fragmentaci√≥n de texto cuando hay <strong>,
  <em>, etc. dentro de los p√°rrafos


    if cuerpo:
        cuerpo = " ".join(cuerpo.split())

‚Üí Si hay cuerpo, normaliza espacios en blanco
‚Üí .split() ‚Üí divide por cualquier whitespace (espacios, \n, \t) ‚Üí lista
‚Üí " ".join() ‚Üí une la lista con un solo espacio
‚Üí Resultado: "texto    con\nmuchos\t  espacios" ‚Üí "texto con muchos espacios"


    if titulo and cuerpo:
        yield {
            "url": response.url,
            "titulo": titulo.strip() if titulo else None,
            "fecha": " ".join(fecha.split()) if fecha else None,
            "cuerpo": cuerpo,
        }

‚Üí VALIDACI√ìN: solo devuelve el item si tiene t√≠tulo Y cuerpo
‚Üí yield ‚Üí devuelve un diccionario (item) al motor de Scrapy
‚Üí Scrapy lo procesa seg√∫n settings.py y lo exporta al CSV


================================================================================
4. SETTINGS ELEGIDOS - EXPLICACI√ìN
================================================================================

BOT_NAME = "noticias_econ"
--------------------------
‚Üí Nombre identificador del proyecto
‚Üí Se usa internamente por Scrapy


SPIDER_MODULES y NEWSPIDER_MODULE
----------------------------------
‚Üí Indica d√≥nde Scrapy debe buscar los spiders
‚Üí SPIDER_MODULES: d√≥nde est√°n los spiders existentes
‚Üí NEWSPIDER_MODULE: d√≥nde crear nuevos spiders con `scrapy genspider`


ROBOTSTXT_OBEY = True
---------------------
‚Üí ‚≠ê √âTICA DE SCRAPING: respeta el archivo robots.txt del sitio
‚Üí Si ambito.com tiene reglas que proh√≠ben scrapear ciertas p√°ginas, las respeta
‚Üí SIEMPRE d√©jalo en True a menos que tengas permiso expl√≠cito del sitio


CONCURRENT_REQUESTS = 16 (comentado, default)
----------------------------------------------
‚Üí N√∫mero de requests simult√°neos que Scrapy puede hacer
‚Üí Default: 16 (buen balance entre velocidad y no saturar el servidor)
‚Üí ‚ö†Ô∏è No aumentes mucho este valor o podr√≠as ser bloqueado


CONCURRENT_REQUESTS_PER_DOMAIN = 1
-----------------------------------
‚Üí ‚≠ê CLAVE PARA NO SER BLOQUEADO
‚Üí Solo 1 request a la vez a ambito.com
‚Üí Evita saturar el servidor y parecer un bot malicioso
‚Üí El scraping ser√° m√°s lento pero m√°s seguro


DOWNLOAD_DELAY = 1
------------------
‚Üí ‚≠ê ESPERA 1 SEGUNDO entre cada request
‚Üí Simula comportamiento humano
‚Üí Reduce la carga en el servidor
‚Üí Disminuye probabilidad de ser bloqueado
‚Üí Puedes aumentarlo a 2-3 si quieres ser m√°s conservador


custom_settings - FEEDS
------------------------
‚Üí Configuraci√≥n de exportaci√≥n de datos

'FEEDS': {
    'noticias.csv': {
        'format': 'csv',              ‚Üí Formato de salida
        'encoding': 'utf-8',          ‚Üí Para caracteres especiales (√±, √°, etc.)
        'store_empty': False,         ‚Üí No crea archivo si no hay items
        'fields': [...],              ‚Üí Orden de las columnas en el CSV
        'overwrite': True,            ‚Üí Sobreescribe el archivo si existe
    }
}

‚Üí Esto SIMPLIFICA la ejecuci√≥n: no necesitas especificar -o noticias.csv


'FEED_EXPORT_FIELDS': ['url', 'titulo', 'fecha', 'cuerpo']
-----------------------------------------------------------
‚Üí Garantiza que el CSV tenga estas columnas EN ESTE ORDEN
‚Üí Importante para consistencia cuando proceses los datos despu√©s


================================================================================
5. C√ìMO EJECUTAR EL BOT
================================================================================

PASO 1: Abre la terminal
-------------------------

PASO 2: Navega al directorio del PROYECTO (donde est√° scrapy.cfg)
------------------------------------------------------------------

Si est√°s en tu carpeta de usuario:

    Windows:
    cd C:\Users\TuUsuario\ruta\al\proyecto\noticias_econ

    Mac/Linux:
    cd ~/ruta/al/proyecto/noticias_econ


‚ö†Ô∏è IMPORTANTE: Debes estar en la carpeta que CONTIENE el archivo scrapy.cfg
   y la subcarpeta noticias_econ/

Verifica que est√°s en el lugar correcto:

    Windows:
    dir

    Mac/Linux:
    ls

Deber√≠as ver:
    scrapy.cfg
    noticias_econ/


PASO 3: Ejecuta el spider
--------------------------

Comando b√°sico:

    scrapy crawl ambito


Con el settings.py configurado, esto autom√°ticamente:
‚úì Scrapear√° todas las p√°ginas de econom√≠a
‚úì Exportar√° a noticias.csv en la carpeta actual
‚úì Respetar√° el delay de 1 segundo entre requests


COMANDOS OPCIONALES:
--------------------

- Ver m√°s informaci√≥n durante el scraping:

    scrapy crawl ambito --loglevel=INFO


- Ver TODO el detalle (debugging):

    scrapy crawl ambito --loglevel=DEBUG


- Exportar a otro archivo (sobrescribe configuraci√≥n):

    scrapy crawl ambito -o otro_nombre.csv


- Exportar a JSON en lugar de CSV:

    scrapy crawl ambito -o noticias.json


- Limitar p√°ginas (√∫til para pruebas):

    scrapy crawl ambito -a max_pages=3

  (Nota: necesitar√≠as agregar esta funcionalidad al c√≥digo)

  codigo modificado para que scrapee solo 3 p√°ginas:
  def __init__(self, max_pages=None, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.current_page = 0
    # Si no se especifica max_pages, usa un valor muy alto (infinito efectivo)
    self.max_pages = int(max_pages) if max_pages else 999999

def parse(self, response):
    news_links = response.css("h2.news-article__title a::attr(href)").getall()
    self.logger.info(f"P√°gina {self.current_page}: {len(news_links)} links encontrados")

    if not news_links:
        self.logger.info("No hay m√°s noticias, finalizando scraper")
        return

    for link in news_links:
        yield response.follow(link, callback=self.parse_noticia)

    # ‚≠ê Agregar condici√≥n de max_pages
    self.current_page += 1
    if self.current_page < self.max_pages:  # ‚≠ê ESTA ES LA L√çNEA CLAVE
        next_page_url = f"https://www.ambito.com/economia/{self.current_page}"
        self.logger.info(f"Avanzando a la p√°gina {self.current_page}")
        yield scrapy.Request(next_page_url, callback=self.parse)
    else:
        self.logger.info(f"L√≠mite de {self.max_pages} p√°ginas alcanzado")


PASO 4: Espera a que termine
-----------------------------

El spider se detendr√° autom√°ticamente cuando encuentre una p√°gina sin noticias.

Ver√°s en la terminal:
    [ambito] INFO: No hay m√°s noticias, finalizando scraper
    [scrapy.core.engine] INFO: Closing spider (finished)


PASO 5: Encuentra tu archivo
-----------------------------

El archivo noticias.csv estar√° en el mismo directorio donde ejecutaste el comando.


================================================================================
6. TROUBLESHOOTING
================================================================================

PROBLEMA: "ModuleNotFoundError: No module named 'noticias_econ'"
SOLUCI√ìN: Est√°s en el directorio incorrecto. Ve a la carpeta que contiene
          scrapy.cfg


PROBLEMA: "Spider not found: ambito"
SOLUCI√ìN: Verifica que el archivo ambito.py est√© en noticias_econ/spiders/
          y que name = "ambito" est√© correctamente definido


PROBLEMA: El scraping es muy lento
SOLUCI√ìN: Es normal por DOWNLOAD_DELAY = 1. Puedes reducirlo a 0.5 pero
          aumenta el riesgo de bloqueo.


PROBLEMA: No se exporta el CSV
SOLUCI√ìN: Verifica que hay items scrapeados. Si todas las noticias fallan
          la validaci√≥n (no tienen t√≠tulo o cuerpo), no se crea el archivo.


PROBLEMA: Caracteres raros en el CSV (√É¬±, √É¬©, etc.)
SOLUCI√ìN: Abre el CSV con un editor que soporte UTF-8 (VS Code, Sublime)
          o en Excel: Datos > Desde texto > selecciona UTF-8


================================================================================
7. PR√ìXIMOS PASOS
================================================================================

- Agregar m√°s campos: categor√≠a, autor, im√°genes
- Implementar pipelines para guardar en base de datos (MongoDB, PostgreSQL)
- Agregar manejo de errores m√°s robusto
- Implementar rotaci√≥n de user-agents
- Scrapear otras secciones: pol√≠tica, deportes, etc.
- Programar ejecuciones autom√°ticas (cron, Task Scheduler)


================================================================================
AUTOR: [Tu nombre]
FECHA: Enero 2026
VERSI√ìN: 1.0
================================================================================

Tutorial scraping en la Virtual Machine

```markdown
# Gu√≠a paso a paso (VM DigitalOcean + Scrapy + export a CSV + tmux)

Este README documenta **todo el proceso completo** para correr un spider de Scrapy en una **Virtual Machine (Droplet) de DigitalOcean** y exportar los resultados a un **CSV descargable**, con monitoreo y sin que el proceso se corte si cerr√°s el navegador.

---

## 0) Idea general (qu√© estamos haciendo)

- Tenemos un proyecto Scrapy versionado en GitHub (`noticias_ambito`).
- Lo clonamos en una VM Ubuntu.
- Creamos un **entorno virtual** (`.venv`) e instalamos dependencias.
- Corremos `scrapy crawl ...` exportando a un CSV.
- Usamos `tmux` para que el proceso siga corriendo aunque te desconectes.
- Bajamos el CSV a tu computadora al final.

---

## 1) Conectarte a la VM (Droplet)

Pod√©s entrar por:
- la **Web Console** de DigitalOcean (lo que estabas usando), o
- por SSH desde tu computadora (m√°s c√≥modo para el futuro).

En la consola vas a ver un prompt tipo:

```

root@ubuntu-...:~#

````

---

## 2) Actualizar el sistema (recomendado)

En Ubuntu, primero actualiz√°s el √≠ndice de paquetes y luego aplic√°s upgrades:

```bash
apt update
apt upgrade -y
````

**Nota:** si el upgrade tarda, es normal.

---

## 3) Instalar herramientas esenciales

Instalamos lo m√≠nimo para trabajar:

* `git`: para clonar el repo
* `python3-venv`: para crear un entorno virtual
* `python3-pip`: para instalar paquetes Python
* `tmux`: para mantener el proceso corriendo aunque cierres la sesi√≥n

```bash
apt install -y git python3-venv python3-pip tmux
```

Chequeos (opcionales):

```bash
git --version
python3 --version
pip3 --version
tmux -V
```

---

## 4) Clonar el repositorio desde GitHub

Nos paramos en home (`~`) y clonamos:

```bash
cd ~
git clone https://github.com/santiagolago/noticias_ambito.git
```

Entramos a la carpeta del proyecto:

```bash
cd noticias_ambito
```

**Chequeo importante:** ac√° deber√≠as ver `scrapy.cfg` y la carpeta del proyecto.

```bash
ls
```

Ten√©s que ver algo como:

* `scrapy.cfg`
* `noticias_econ/`
* `.gitignore`
* `Readme` (o similar)

---

## 5) Crear un entorno virtual (venv) y activarlo

Creamos el entorno virtual dentro del proyecto:

```bash
python3 -m venv .venv
```

Activamos el entorno:

```bash
source .venv/bin/activate
```

**Chequeo visual clave:** el prompt deber√≠a empezar con `(.venv)`, por ejemplo:

```
(.venv) root@ubuntu-...:~/noticias_ambito#
```

---

## 6) Instalar dependencias (requirements)

### Caso A: existe `requirements.txt`

Si el repo tiene `requirements.txt`:

```bash
pip install -r requirements.txt
```

### Caso B: no existe `requirements.txt`

Instal√°s al menos Scrapy:

```bash
pip install scrapy
```

Chequeo de Scrapy:

```bash
scrapy version
```

---

## 7) Entender por qu√© usamos tmux (MUY importante)

Cuando corr√©s un proceso ‚Äúpesado‚Äù (como un scraping largo) en una terminal web o sesi√≥n SSH, si cerr√°s el navegador o se corta Internet, el proceso **puede cortarse**.

**tmux** crea una ‚Äúsesi√≥n persistente‚Äù dentro de la VM:

* el scraping sigue corriendo aunque cierres la pesta√±a,
* pod√©s volver despu√©s y ver el progreso.

---

## 8) Crear/entrar a una sesi√≥n tmux

Creamos una sesi√≥n llamada `ambito`:

```bash
tmux new -s ambito
```

A partir de ac√°, est√°s *dentro* de tmux.

---

## 9) Regla cr√≠tica: activar el venv DENTRO de tmux

Aunque ya lo hayas activado antes, **tmux es otra sesi√≥n**.
Por eso, dentro de tmux repetimos:

```bash
cd ~/noticias_ambito
source .venv/bin/activate
```

Si no hac√©s esto, vas a ver errores como:

```
scrapy: command not found
```

Chequeo r√°pido:

```bash
which scrapy
scrapy version
```

---

## 10) Correr el spider exportando a CSV y guardando log

Este comando:

* corre el spider `ambito`,
* guarda el output en `ambito_economia.csv`,
* usa encoding amigable para Excel (`utf-8-sig`),
* guarda el log completo en `ambito_run.log`,
* y adem√°s te lo muestra en pantalla.

```bash
scrapy crawl ambito -O ambito_economia.csv -s FEED_EXPORT_ENCODING=utf-8-sig 2>&1 | tee ambito_run.log
```

### Qu√© significa cada parte

* `scrapy crawl ambito`: corre el spider cuyo `name = "ambito"`
* `-O ambito_economia.csv`: exporta a CSV (si ya existe, lo pisa)
* `-s FEED_EXPORT_ENCODING=utf-8-sig`: tildes/√± bien en Excel
* `2>&1`: manda errores (stderr) al mismo stream que output
* `| tee ambito_run.log`: muestra por pantalla y tambi√©n guarda log a archivo

---

## 11) C√≥mo saber si est√° funcionando mientras corre

Vas a ver l√≠neas en vivo del tipo:

* `Crawled (200) <GET ...>`
* `Scraped from <200 ...>`
* estad√≠sticas peri√≥dicas

Pod√©s monitorear el archivo CSV desde otra terminal o desde otra ventana tmux.

### Opci√≥n: abrir otra ventana dentro de tmux

* Nueva ventana: `Ctrl + b` y luego `c`
* Volver: `Ctrl + b` y luego `0` o `1`

En esa ventana:

```bash
ls -lh ambito_economia.csv
tail -n 2 ambito_economia.csv
```

Esto te dice si:

* el archivo existe,
* aumenta de tama√±o,
* se est√° escribiendo.

---

## 12) C√≥mo ‚Äúsalir‚Äù sin cortar el scraping (detach)

**NO uses Ctrl+C** si quer√©s que siga corriendo.

Para dejarlo corriendo en segundo plano (sin matarlo):

* Presion√° `Ctrl + b`
* solt√°s
* presion√° `d`

Eso se llama **detach**.

‚úÖ El proceso queda corriendo en la VM
‚úÖ Pod√©s cerrar el navegador y no pasa nada

---

## 13) Volver a entrar a la sesi√≥n tmux m√°s tarde

En cualquier momento, desde la VM:

```bash
tmux attach -t ambito
```

Volv√©s exactamente a donde estaba corriendo Scrapy.

---

## 14) Qu√© NO hacer (alertas importantes)

* ‚ùå **No hagas `Ctrl + C` dentro del tmux**: eso corta el scraping.
* ‚ùå No reinicies/apagues el droplet si quer√©s que siga.
* ‚ùå No cierres tmux con `exit` si est√°s en la ventana donde corre Scrapy (pod√©s cortar la sesi√≥n).

Lo correcto es:

* ‚úÖ `Ctrl+b` luego `d` (detach)

---

## 15) C√≥mo saber que termin√≥

Cuando termine, Scrapy imprime algo como:

* `Spider closed (finished)`
* y un bloque grande de estad√≠sticas finales.

En ese punto, el CSV ya qued√≥ en disco.

---

## 16) Descargar el CSV a tu computadora (con scp)

El CSV est√° en la VM, por ejemplo:

```
/root/noticias_ambito/ambito_economia.csv
```

Desde tu computadora (no desde la VM), ejecut√°s:

```bash
scp root@TU_IP_PUBLICA:/root/noticias_ambito/ambito_economia.csv .
```

* `TU_IP_PUBLICA` es la IP del droplet (la ves en DigitalOcean).
* El `.` lo guarda en el directorio actual de tu PC.

En Windows, si quer√©s bajarlo a Descargas:

```bash
scp root@TU_IP_PUBLICA:/root/noticias_ambito/ambito_economia.csv C:\Users\santi\Downloads\
```

---

## 17) Revisar logs si algo falla

Si el proceso se corta o quer√©s ver el log:

```bash
tail -n 100 ambito_run.log
```

O buscar errores:

```bash
grep -i "error" ambito_run.log | tail -n 50
```

---

## 18) Nota final (muy importante): paginaci√≥n completa

Este README documenta c√≥mo **correr y exportar**.

Pero para scrapeo ‚Äútotal‚Äù de la secci√≥n econom√≠a, tu spider tiene que:

* seguir p√°ginas como `/economia/1`, `/economia/2`, etc.
* hasta que no haya m√°s p√°ginas.

Si tu c√≥digo no tiene esa l√≥gica, solo va a scrapear la primera p√°gina.

---

## Mini resumen operativo

1. `tmux new -s ambito`
2. `cd ~/noticias_ambito && source .venv/bin/activate`
3. correr el comando Scrapy con `tee`
4. para dejarlo corriendo: `Ctrl+b` `d`
5. para volver: `tmux attach -t ambito`
6. para bajar CSV: `scp ...`

---

```
```

